{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4e4e5-d05c-46ac-8bf8-6222df71eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "YT_URL= \"https://www.youtube.com/watch?v=BZbGwnZ6UME\"\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    YT_URL, language=[\"en\", \"es\", \"de\"], translation=\"de\", add_video_info=True,\n",
    ")\n",
    "\n",
    "transcript = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6971a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d72f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transcript[0].page_content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz.\n",
    "You are given a quiz question, the true answer, three incorrect answers, the context from which the question was taken, and a short summary of the topic for which the quiz is intended. Your task is to grade the quiz question based on the following criteria:\n",
    "\n",
    "1. **Clear and concise**: The question and answers should be easy to understand, well-formulated, and free from ambiguity.\n",
    "2. **Engaging and challenging**: Incorrect answers should be plausible, making the quiz engaging and requiring thoughtful consideration.\n",
    "3. **Relatively short**: Answers should be brief, typically no more than a sentence.\n",
    "4. **Accurately aligned with the specified difficulty level**: The question should match the intended difficulty level. For **easy** quizzes, questions should focus on basic, easily recognizable information. For **medium** quizzes, questions should involve more complex topics requiring a deeper understanding. For **hard** quizzes, questions should require critical thinking, analysis, or interpretation of nuanced information, with particularly convincing incorrect options.\n",
    "5. *Relevant to the topic**: The quiz question should be relevant to the provided topic, ensuring that it refers to the overall theme of the video. If the question does not refer to the topic as described in the provided video summary, the grade should be 0.00.\n",
    "6. **Quiz question in correct language**: It is highly important that the quiz question is in the correct language, in this case the quiz language is **{language}** ('EN'=English, 'ES'=Spanish, 'DE'=German). If the question is not in the specified language, give it a grade of 0.00.\n",
    "\n",
    "GRADE: (range from 0.00 - 4.00) - A grade of '0' means the question does not fulfill any criteria and may not reflect the provided context well. A grade of '4' means the question fully meets all criteria. The more criteria met, the higher the grade should be, up to a maximum of 4. The grade should be a floating point number with two decimal places.\n",
    "\n",
    "Ignore issues in punctuation. Only provide the grade, no reasoning is required.\n",
    "\n",
    "\\n**QUESTION**: {question}\n",
    "\\n**CORRECT ANSWER**: {correct_answer}\n",
    "\\n**INCORRECT ANSWERS**: {incorrect_answers}\n",
    "\\n**DIFFICULTY**: {difficulty}\n",
    "\\n**CONTEXT**: {context}\n",
    "\\n**TOPIC SUMMARY**: {summary}\n",
    "\\n>>GRADE<<:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\", \"correct_answer\", \"incorrect_answers\", \"difficulty\", \"context\", \"summary\", \"language\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "def get_qa_data(quiz_question):\n",
    "    \n",
    "    answers = quiz_question.metadata[\"answer\"]\n",
    "    return {\"question\": quiz_question.page_content, \"correct_answer\": answers[\"correct_answer\"], \"incorrect_answers\": \" ### \".join(answers[\"wrong_answers\"]), \"difficulty\": \"HARD\", \"context\": quiz_question.metadata[\"context\"], \"summary\": dark_matter_summary, \"language\": \"ES\"}\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-4o-mini\",\n",
    "), prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_converted = [(i, float(grade)) for (i,grade) in vals if float(grade) > 0.00 ]\n",
    "vals_converted\n",
    "\n",
    "sorted_vals = sorted(vals_converted, key=lambda grade: grade[1], reverse=True)\n",
    "sorted_vals\n",
    "\n",
    "qa_list = [qa_list[i] for i, grade in sorted_vals[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe660db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "qa_list = dark_matter_list\n",
    "\n",
    "for i in range(len(qa_list)):\n",
    "    val = llm_chain.run(**get_qa_data(qa_list[i]))\n",
    "    vals.append((i, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a002b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript[0].metadata\n",
    "\n",
    "video_metadata = dict(**transcript[0].metadata)\n",
    "video_metadata.update({\"transcript\": transcript[0].page_content})\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "doc = Document(page_content=video_metadata[\"transcript\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b77c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "templ1 = \"\"\"You are a highly intelligent and insightful assistant designed to generate high-quality multiple-choice quiz questions. Your task is to read the provided text, identify key information, and create a corresponding question with four possible answersâ€”one correct answer and three plausible but incorrect alternatives.\n",
    "\n",
    "Each question and its associated answers should be:\n",
    "1. **Clear and concise**: Ensure that both the question and the answers are easy to understand, well-formulated, and free from ambiguity.\n",
    "2. **Engaging and challenging**: The incorrect answers should be plausible enough to make the quiz engaging, requiring thoughtful consideration by the user.\n",
    "3. **Relatively short**: Answers should be brief, typically no more than a sentence.\n",
    "4. **Appropriately aligned with the specified difficulty level**: Ensure the generated questions match the intended difficulty. For **easy** quizzes, the questions should focus on basic, easily recognizable information with straightforward answers. For **medium** quizzes, questions should involve more complex topics requiring a deeper understanding or application of the subject. For **hard** quizzes, craft questions that require critical thinking, analysis, or interpretation of nuanced information. The incorrect options should be particularly convincing, making the quiz genuinely challenging at this level.\n",
    "\n",
    "When generating these question-answer pairs, follow this format:\n",
    "\n",
    "```\n",
    "{{\n",
    "\"question\": \"$YOUR_QUESTION_HERE\",\n",
    "\"answer\": {{\"correct_answer\": \"$THE_CORRECT_ANSWER_HERE\", \"wrong_answers\": [\"$THE_FIRST_WRONG_ANSWER_HERE\",\"$THE_SECOND_WRONG_ANSWER_HERE\",\"$THE_THIRD_WRONG_ANSWER_HERE\"]}}\n",
    "}}\n",
    "```\n",
    "\n",
    "Everything between the ``` must be valid JSON, make sure there are no whitespaces or special characters in the output so that it can be parsed to JSON!\n",
    "\n",
    "Here is an example of how to extract question/answers from a given text:\n",
    "> Given Text:\n",
    "\"Albert Einstein developed the theory of relativity, one of the two pillars of modern physics. His work also laid the foundation for the development of quantum mechanics.\"\n",
    "> Generated question and answers:\n",
    "\"question\": \"Who developed the theory of relativity?\",\n",
    "\"correct_answer\": \"Albert Einstein\",\n",
    "\"wrong_answers\": [\"Isaac Newton\",\"Niels Bohr\",\"Marie Curie\"]\n",
    "> Additionally, the question/answers should be formated to JSON format as stated above.\n",
    "\n",
    "You still have to parse the question and answers into a valid JSON as provided above!\n",
    "\n",
    "\"\"\"\n",
    "templ2 = \"\"\"Please come up with question/answers pair in JSON format from the following given text with the conditions that:\n",
    "\n",
    "1. The quiz questions should be of difficulty **{difficulty}**.\n",
    "2. The quiz questions should be in the same language as the text, in this case in **{language}**.\n",
    "3. The following text is only a transcript of a YouTube video, refer to it as 'video' not as 'text'.\n",
    "4. Please extract AT LEAST ONE adquate quiz question from the text, remember to provide it in the mentioned JSON format!\n",
    "\n",
    "This is your attempt number {num_attempt} in generating a quiz question of this text. If you already tried more than once it means you failed to properly retrieve a quiz question in the necessary JSON format. If that is the case, please vehemently try to parse the quiz question in the given JSON format!\n",
    "\n",
    "Here is the text:\n",
    "----------------\n",
    "{text}\"\"\"\n",
    "\n",
    "\n",
    "def get_qa_prompt(difficulty: str, language: str, num_attempt: int):\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(templ1),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                templ2,\n",
    "                partial_variables={\"difficulty\": difficulty, \"language\": language, \"num_attempt\": num_attempt},\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "get_qa_prompt(difficulty=\"HARD\", language=\"ES\", num_attempt=1).format(text=\"mytext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following text and in the same language:\n",
    "    \"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm_chain = LLMChain(llm=ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-4o-mini\",\n",
    "), prompt=prompt)\n",
    "\n",
    "llm_chain.run(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481408f-cf85-46a8-92a6-f8bd86a43b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.qa_generation.base import QAGenerationChain\n",
    "\n",
    "import itertools\n",
    "import uuid\n",
    "import sys\n",
    "from pathlib import Path\n",
    "    \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(Path.cwd())\n",
    "\n",
    "from json import JSONDecodeError\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"POSTGRES_DRIVER\"] = \"psycopg\"\n",
    "os.environ[\"POSTGRES_HOST\"]=\"localhost\"\n",
    "os.environ[\"POSTGRES_PORT\"]=\"5432\"\n",
    "os.environ[\"POSTGRES_DATABASE\"] = \"quizstream_db\"\n",
    "os.environ[\"POSTGRES_USER\"] = \"admin\"\n",
    "os.environ[\"POSTGRES_PASSWORD\"] = \"my_password\"\n",
    "\n",
    "def chunk_transcript(doc: Document) -> list[Document]:\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=len(doc.page_content) // 7,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "    # add end_index\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"end_index\"] = chunk.metadata[\"start_index\"] + len(\n",
    "            chunk.page_content\n",
    "        )\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_qa_from_chunk(\n",
    "    chunk: Document,\n",
    "    qa_generator_chain: QAGenerationChain,\n",
    ") -> list[dict]:\n",
    "    try:\n",
    "        # return list of qa pairs\n",
    "        qa_pairs = qa_generator_chain.run(chunk.page_content)\n",
    "\n",
    "        # attach chunk metadata to qa_pair\n",
    "        for qa_pair in qa_pairs:\n",
    "            qa_pair[\"metadata\"] = dict(**chunk.metadata)\n",
    "            qa_pair[\"metadata\"].update(\n",
    "                {\"id\": str(uuid.uuid4()), \"context\": chunk.page_content}\n",
    "            )\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    except JSONDecodeError:\n",
    "        return [-1]\n",
    "\n",
    "def generate_qa_from_transcript(transcript: Document) -> list[dict[str, str]]:\n",
    "\n",
    "    chunks = chunk_transcript(transcript)\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    qa_chain = QAGenerationChain.from_llm(llm, prompt=get_qa_prompt(difficulty=\"HARD\", language=\"EN\", num_attempt=1))\n",
    "\n",
    "    qa_pairs = [get_qa_from_chunk(chunk, qa_chain) for chunk in chunks]\n",
    "    qa_pairs = list(itertools.chain.from_iterable(qa_pairs))\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "data = generate_qa_from_transcript(transcript[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "qa_chain = QAGenerationChain.from_llm(llm, prompt=get_qa_prompt(difficulty=\"HARD\", language=\"EN\", num_attempt=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain.llm_chain.prompt = get_qa_prompt(difficulty=\"HARD\", language=\"EN\", num_attempt=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain.llm_chain.prompt.messages[1].prompt.partial_variables.update({\"num_attempt\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain.llm_chain.prompt.format(text=\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fe615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53a63c-5945-4fa5-8f71-2cfd801dc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.quiz_generation.generator import agenerate_quiz\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(), override=True)\n",
    "\n",
    "col_metadata, qa_ids = await agenerate_quiz(\"my_xyz_quiz\",\n",
    "                                YT_URL,\n",
    "                                {\"OPENAI_API_KEY\": os.environ.get(\"OPENAI_API_KEY\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8debfa-cdbc-41f0-abf7-62e342d835c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection_string = \"postgresql+psycopg://postgres:pwd@localhost:5432/quizzes\"  # Uses psycopg3!\n",
    "collection_name = \"my_docs\"\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "embeddings = FakeEmbeddings(size=1)\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection_string,\n",
    "    use_jsonb=True,\n",
    "    collection_metadata={\"date_created\": dt.datetime.now(dt.UTC).strftime('%Y-%m-%dT%H:%M:%SZ'), 'num_tries': 1, 'acc':0.2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c868c-1f25-4252-85bc-c1364803263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "\n",
    "TABLE_COLLECTION = \"langchain_pg_collection\"\n",
    "TABLE_DOCS = \"langchain_pg_embedding\"\n",
    "\n",
    "def list_collections() -> list[str]:\n",
    "    # Create an engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Reflect the specific table\n",
    "    metadata = MetaData()\n",
    "    table = Table(TABLE_COLLECTION, metadata, autoload_with=engine)\n",
    "\n",
    "    # Query the column\n",
    "    query = select(table.c[\"name\"])\n",
    "    with engine.connect() as connection:\n",
    "        results = connection.execute(query).fetchall()\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_by_ids(ids: list[str]) -> list[str]:\n",
    "    # Create an engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Reflect the specific table\n",
    "    metadata = MetaData()\n",
    "    table = Table(TABLE_DOCS, metadata, autoload_with=engine)\n",
    "\n",
    "    # Query the column\n",
    "    query = select(table).where(table.c.id.in_(ids))\n",
    "    with engine.connect() as connection:\n",
    "        results = connection.execute(query).fetchall()\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_all_by_collection_id(engine:Engine, collection_id: str):\n",
    "\n",
    "    # Reflect the specific table\n",
    "    table = Table(TABLE_DOCS, MetaData(), autoload_with=engine)\n",
    "\n",
    "    # Query the column\n",
    "    query = select(table).where(table.c.collection_id == collection_id)\n",
    "    with engine.connect() as connection:\n",
    "        results = connection.execute(query).fetchall()\n",
    "\n",
    "    return results\n",
    "\n",
    "list_collections(), get_by_ids([\"aad30221-cdc1-4add-86db-f4067ee9d8c7\"]), get_all_by_collection_id(engine, \"994272b7-354d-4c98-9c92-8d4b23b60e62\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
